{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-AhTVANzVH8"
   },
   "source": [
    "# Business Objective: Online Course Recommendation System\n",
    "## Problem Statement\n",
    "The goal of this dataset is to build an online course recommendation system that suggests relevant courses to learners based on their interests, past enrollments, and engagement levels. The dataset includes course ratings, instructor information, previous learning history, study material availability, and certification offerings, making it suitable for recommendation models using collaborative filtering, content-based filtering, or hybrid approaches.\n",
    "\n",
    "## Variable Descriptions\n",
    "    Variable Name -- Data Type -- Description\n",
    "    user_id -- Integer -- Unique identifier for each learner.\n",
    "    course_id -- Integer -- Unique identifier for each online course.\n",
    "    course_name -- String -- The name of the online course.\n",
    "    instructor -- String -- The name of the instructor teaching the course.\n",
    "    course_duration_hours -- Float (5.0 - 100.0) -- The duration of the course in hours.\n",
    "    certification_offered -- String (Yes/No) -- Indicates whether the course provides a certification upon completion.\n",
    "    difficulty_level -- String -- The difficulty level of the course (Beginner, Intermediate, Advanced).\n",
    "    rating -- Float (1.0 - 5.0) -- User-provided rating for the course.\n",
    "    enrollment_numbers -- Integer -- The total number of students enrolled in the course.\n",
    "    course_price -- Float (20.0 - 500.0) -- The price of the online course.\n",
    "    feedback_score -- Float (0.0 - 1.0) -- A normalized score representing the feedback sentiment from students.\n",
    "    study_material_available -- String (Yes/No) -- Indicates whether additional study materials are available.\n",
    "    time_spent_hours -- Float (1.0 - 100.0) -- The average time spent by students in the course (in hours).\n",
    "    previous_courses_taken -- Integer -- The number of previous courses the learner has taken before enrolling in this one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCmZqB2hzGZk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Uk-V6AdpLRo",
    "outputId": "6630fc78-ab3d-4e2d-9ed6-015fe7e97fa5"
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "KGDWoYO8ytjm",
    "outputId": "174ac3e8-87ae-4205-a8b0-f4a96ace89af"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded= files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "yVY5NnYgy-aK",
    "outputId": "6af8f44b-52a0-4f8a-a39d-ac86beb3ff50"
   },
   "outputs": [],
   "source": [
    "df= pd.read_excel('online_course_recommendation_v2 (1).xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kp-fVNE870WG",
    "outputId": "a3afa360-8ca7-485b-ba46-15937bf0293a"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_CQnL1u72XN",
    "outputId": "03cb46ce-7db1-48c2-cdf2-aaeb22fbed68"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "vKf2lEEF5Eqv",
    "outputId": "65985c8c-8286-48b3-fd35-d0fd2213b4ef"
   },
   "outputs": [],
   "source": [
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "100MUYhPNstw",
    "outputId": "bdaa771b-fe18-4e5c-871c-f6ccd30499c6"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpi-YgzZ5JWG"
   },
   "outputs": [],
   "source": [
    "df['course_taken']= df.groupby('user_id')['user_id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "e4nZECxry-Wu",
    "outputId": "8208b732-64da-4aa2-b5f2-a1d7d835fea4"
   },
   "outputs": [],
   "source": [
    "df1= df.sort_values(by= 'user_id', ascending= True).reset_index(drop= True)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-opnPIdXs33D"
   },
   "outputs": [],
   "source": [
    "df.drop('previous_courses_taken', axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 930
    },
    "id": "hrenqvE4boLr",
    "outputId": "2fcebcb3-fc2f-4ae8-87c9-24f054d7b618"
   },
   "outputs": [],
   "source": [
    "df.hist(bins=30, figsize=(15,10))\n",
    "plt.suptitle(\"Distribution of Numeric Columns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRUQ94ZOy-NA"
   },
   "outputs": [],
   "source": [
    "# # Selecting numeric columns\n",
    "# numeric_cols= df.select_dtypes(include= ['float64', 'int64']).columns\n",
    "\n",
    "# df_clean= df.copy()\n",
    "\n",
    "# # Loop through each num cols\n",
    "# for col in numeric_cols:\n",
    "#   Q1= df[col].quantile(0.25)\n",
    "#   Q3= df[col].quantile(0.75)\n",
    "#   IQR= Q3- Q1\n",
    "#   lower= Q1 - 1.5 * IQR\n",
    "#   upper= Q3 + 1.5 * IQR\n",
    "#   outliers= df[(df[col] < lower) | (df[col] > upper)]\n",
    "#   print(f\"{col}: {len(outliers)} outliers\")\n",
    "\n",
    "#   # Keep ionly rows within bounds\n",
    "#   df_clean= df_clean[(df_clean[col] >= lower) & (df_clean[col] <= upper)]\n",
    "\n",
    "# print('original shape: ', df.shape)\n",
    "# print('After removing outliers:', df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IFUxXvny-JX"
   },
   "outputs": [],
   "source": [
    "# df_clean.hist(bins=30, figsize=(15,10))\n",
    "# plt.suptitle(\"Distribution of Numeric Columns\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXq4HyjSjqAK"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "wQziBTFJn9ys",
    "outputId": "d386a5b2-64d8-453e-a8e2-fe32f4f0217e"
   },
   "outputs": [],
   "source": [
    "# 1. Distribution of Course Ratings\n",
    "sns.histplot(df['rating'], bins=20, kde=True)\n",
    "plt.title('Distribution of Course Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "neDwwS47ICbD",
    "outputId": "d2694a6b-bda4-4d67-a508-9fc14b0c8b3d"
   },
   "outputs": [],
   "source": [
    "# 2. Difficulty levels\n",
    "sns.countplot(x='difficulty_level', data= df)\n",
    "plt.title('Count of Courses by Difficulty Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "QHsc2aKMI-fP",
    "outputId": "22f1b917-d572-4bd0-e2e4-757a3e559d7e"
   },
   "outputs": [],
   "source": [
    "# 3. Certification offered\n",
    "df['certification_offered'].value_counts().plot.pie(autopct= '%1.1f%%', colors= ['cornflowerblue', 'salmon'])\n",
    "plt.title('Certification Offered: Yes/No')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "048r3s4Mh6JC",
    "outputId": "3a5aff12-2d27-4dbc-9ec6-37065fb55811"
   },
   "outputs": [],
   "source": [
    "# 4. Study Material Available\n",
    "sns.countplot(x= 'study_material_available', data= df)\n",
    "plt.title('Study Material Availability Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "lFiWjVJzKN8S",
    "outputId": "8b773fea-8fc2-4e6c-84d1-0f2669cdf99f"
   },
   "outputs": [],
   "source": [
    "# 5. Top 20 Instructors by Number Of Courses\n",
    "df['instructor'].value_counts().head(20).plot(kind= 'barh', figsize= (8,6), color= 'teal')\n",
    "plt.title('Top 20 Instructors by Number of Courses')\n",
    "plt.xlabel('Number of Courses')\n",
    "plt.ylabel('Instructor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "SMfhYQoXKwV8",
    "outputId": "9ce9e69c-7080-4c8e-f6e9-c33edaad9f66"
   },
   "outputs": [],
   "source": [
    "# 6. Top 10 Most Enrolled Courses\n",
    "top_courses= (df.groupby('course_name')['enrollment_numbers'].sum().sort_values(ascending= False).head(10).reset_index())\n",
    "sns.barplot(x= 'enrollment_numbers', y= 'course_name', data= top_courses)\n",
    "plt.title('Top 10 Most Enrolled Courses')\n",
    "plt.xlabel('Total Enrollments')\n",
    "plt.ylabel('Course Name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzSGk6SKpZLj",
    "outputId": "ce33e739-15ee-4c4d-924b-6b03ae0caae5"
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "nxHPBA4bLk1V",
    "outputId": "97d84de4-39c4-44d4-938e-4bc4f6ad00f5"
   },
   "outputs": [],
   "source": [
    "# 7. Word Cloud for Course Names\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text= ' '.join(df['course_name'].astype(str))\n",
    "wordcloud= WordCloud(width= 800, height= 400, background_color= 'white').generate(text)\n",
    "plt.figure(figsize= (10, 5))\n",
    "plt.imshow(wordcloud, interpolation= 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('WordCloud - Course Names')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "i_Uk2L0uy9Mk",
    "outputId": "cdff58cc-dc6a-47d6-cb54-7dbd426ac7cf"
   },
   "outputs": [],
   "source": [
    "# 7. Word Cloud for Course Names\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text = ' '.join(df['instructor'].astype(str))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap= 'cool').generate(text)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud - Instructors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Vc3LTZGOtIW",
    "outputId": "18975aee-a94c-4dca-96bc-9b2c98806685"
   },
   "outputs": [],
   "source": [
    "# 9. Frequency Tables & Cross-tabs\n",
    "freq_table= df['difficulty_level'].value_counts()\n",
    "print(freq_table)\n",
    "\n",
    "cross_tab= pd.crosstab(df['difficulty_level'], df['certification_offered'])\n",
    "print(cross_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "0wvm15-GPIGF",
    "outputId": "9b31ccbb-f99f-460b-8e11-c4c8b41b7769"
   },
   "outputs": [],
   "source": [
    "# 10. Average Rating by Difficulty level and Instructor\n",
    "plt.figure(figsize= (14, 7))\n",
    "sns.barplot(x= 'difficulty_level', y= 'rating', hue= 'instructor', data= df, ci=None)\n",
    "plt.title('Average Rating by Difficulty level and Instructor')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.xlabel('Difficulty Level')\n",
    "plt.legend(bbox_to_anchor= (1.05, 1), loc= 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "jMiIrmaCQdm9",
    "outputId": "23029baf-49f0-4e57-ac15-f830e8020c00"
   },
   "outputs": [],
   "source": [
    "# 11. Enrollment Numbers by Course Price & Difficulty Level\n",
    "plt.figure(figsize= (8,6))\n",
    "df_pivot= df.pivot_table(values= 'enrollment_numbers', index= 'course_price', columns= 'difficulty_level', aggfunc= 'mean').fillna(0)\n",
    "sns.heatmap(df_pivot, cmap= 'Blues')\n",
    "plt.title('Enrollment Numbers by Course Price & Difficulty Level')\n",
    "plt.xlabel('Difficulty Level')\n",
    "plt.ylabel('Course Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpoacZY0YBRn"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSEvLZ9ayqxT"
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # üì¶ FILE: preprocessing_pipeline.py\n",
    "# # ============================================================\n",
    "# # PURPOSE:\n",
    "# #   1Ô∏è‚É£ Converts raw input data into a clean, model-ready DataFrame.\n",
    "# #   2Ô∏è‚É£ Encodes categorical columns.\n",
    "# #   3Ô∏è‚É£ Scales numerical columns using MinMaxScaler.\n",
    "# #   4Ô∏è‚É£ Saves LabelEncoders and Scaler for consistent transformation.\n",
    "# # ============================================================\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "# import joblib\n",
    "# import os\n",
    "\n",
    "\n",
    "# def prepare_course_data(df: pd.DataFrame, save_dir: str = \"artifacts\") -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Cleans and transforms raw course dataset into model-ready form.\n",
    "\n",
    "#     Steps:\n",
    "#     1. Encode Yes/No binary columns.\n",
    "#     2. Drop unnecessary columns.\n",
    "#     3. Label encode categorical features.\n",
    "#     4. Apply MinMaxScaler to numerical columns.\n",
    "#     5. Save encoders and scaler (for later inference).\n",
    "#     6. Return a clean, processed DataFrame.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     df : pd.DataFrame\n",
    "#         Raw input dataset.\n",
    "#     save_dir : str, optional\n",
    "#         Directory where encoders/scalers are saved (default is 'artifacts').\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         Cleaned and preprocessed dataset ready for model input.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # üßπ Step 1: Encode Yes/No Columns to Binary\n",
    "#     # ------------------------------------------------------------\n",
    "#     if 'certification_offered' in df.columns:\n",
    "#         df['certification_offered'] = df['certification_offered'].replace({'Yes': 1, 'No': 0})\n",
    "#     if 'study_material_available' in df.columns:\n",
    "#         df['study_material_available'] = df['study_material_available'].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # üóëÔ∏è Step 2: Drop Unnecessary Columns\n",
    "#     # ------------------------------------------------------------\n",
    "#     if 'course_id' in df.columns:\n",
    "#         df = df.drop(['course_id'], axis=1)\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # ‚ú® Step 3: Label Encode Categorical Columns\n",
    "#     # ------------------------------------------------------------\n",
    "#     le_course = LabelEncoder()\n",
    "#     le_instructor = LabelEncoder()\n",
    "#     le_difficulty = LabelEncoder()\n",
    "\n",
    "#     df['course_name_enc'] = le_course.fit_transform(df['course_name'])\n",
    "#     df['instructor_enc'] = le_instructor.fit_transform(df['instructor'])\n",
    "#     df['difficulty_level_enc'] = le_difficulty.fit_transform(df['difficulty_level'])\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # üî¢ Step 4: Apply MinMax Scaling to Numeric Columns\n",
    "#     # ------------------------------------------------------------\n",
    "#     numeric_cols = [\n",
    "#         'enrollment_numbers',\n",
    "#         'course_price',\n",
    "#         'course_duration_hours',\n",
    "#         'feedback_score',\n",
    "#         'time_spent_hours',\n",
    "#         'rating',\n",
    "#         'course_taken'\n",
    "#     ]\n",
    "\n",
    "#     scaler = MinMaxScaler()\n",
    "#     df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # üíæ Step 5: Save All Encoders & Scaler for Deployment\n",
    "#     # ------------------------------------------------------------\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "#     joblib.dump(le_course, f\"{save_dir}/le_course.pkl\")\n",
    "#     joblib.dump(le_instructor, f\"{save_dir}/le_instructor.pkl\")\n",
    "#     joblib.dump(le_difficulty, f\"{save_dir}/le_difficulty.pkl\")\n",
    "#     joblib.dump(scaler, f\"{save_dir}/minmax_scaler.pkl\")\n",
    "\n",
    "#     # ------------------------------------------------------------\n",
    "#     # üéØ Step 6: Keep Only Model-Relevant Columns\n",
    "#     # ------------------------------------------------------------\n",
    "#     scaled_df = df[[\n",
    "#         'user_id',\n",
    "#         'course_name_enc',\n",
    "#         'instructor_enc',\n",
    "#         'difficulty_level_enc'\n",
    "#     ] + numeric_cols]\n",
    "\n",
    "#     print(f\"‚úÖ Data preprocessing complete. Artifacts saved to: '{save_dir}'\")\n",
    "#     return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7v5pi0h0NLW"
   },
   "outputs": [],
   "source": [
    "# from preprocessing_pipeline import prepare_course_data\n",
    "\n",
    "# # Apply preprocessing\n",
    "# scaled_df = prepare_course_data(df)\n",
    "# print(scaled_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALgDFK_54aJe"
   },
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# üì¶ FUNCTION: prepare_course_data()\n",
    "# =========================================================\n",
    "# Cleans, encodes, and scales course data for recommendation models.\n",
    "# - Converts Yes/No columns to binary\n",
    "# - Label-encodes categorical features\n",
    "# - Applies MinMaxScaler to numeric features\n",
    "# - Returns a clean, model-ready dataframe\n",
    "# =========================================================\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "def prepare_course_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for CF, CBF, or Hybrid recommendation models.\n",
    "\n",
    "    Steps:\n",
    "    1. Encode binary Yes/No columns.\n",
    "    2. Drop unnecessary identifiers.\n",
    "    3. Label encode categorical text columns.\n",
    "    4. Apply MinMax scaling to numeric columns.\n",
    "    5. Return a clean, ready-to-train DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Encode binary Yes/No\n",
    "    if 'certification_offered' in df.columns:\n",
    "        df['certification_offered'] = df['certification_offered'].replace({'Yes': 1, 'No': 0})\n",
    "    if 'study_material_available' in df.columns:\n",
    "        df['study_material_available'] = df['study_material_available'].replace({'Yes': 1, 'No': 0})\n",
    "\n",
    "    # 2. Drop unnecessary columns\n",
    "    if 'course_id' in df.columns:\n",
    "        df = df.drop(['course_id'], axis=1)\n",
    "\n",
    "    # 3. Label Encoding\n",
    "    le_course = LabelEncoder()\n",
    "    le_instructor = LabelEncoder()\n",
    "    le_difficulty = LabelEncoder()\n",
    "\n",
    "    df['course_name_enc'] = le_course.fit_transform(df['course_name'])\n",
    "    df['instructor_enc'] = le_instructor.fit_transform(df['instructor'])\n",
    "    df['difficulty_level_enc'] = le_difficulty.fit_transform(df['difficulty_level'])\n",
    "\n",
    "    # 4. Scale numerical features\n",
    "    numeric_cols = [\n",
    "        'enrollment_numbers',\n",
    "        'course_price',\n",
    "        'course_duration_hours',\n",
    "        'feedback_score',\n",
    "        'time_spent_hours',\n",
    "        'rating',\n",
    "        'course_taken'\n",
    "    ]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # 5. Final cleaned DataFrame\n",
    "    scaled_df = df[[\n",
    "        'user_id',\n",
    "        'course_name_enc',\n",
    "        'course_name',\n",
    "        'instructor_enc',\n",
    "        'instructor',\n",
    "        'difficulty_level_enc',\n",
    "        'difficulty_level'\n",
    "    ] + numeric_cols]\n",
    "\n",
    "    # ‚úÖ Return both the DataFrame and the preprocessing objects\n",
    "    return scaled_df, le_course, le_instructor, le_difficulty, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwFnTHokKWdl"
   },
   "outputs": [],
   "source": [
    "scaled_df, le_course, le_instructor, le_difficulty, scaler = prepare_course_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "WLWTLLky4aGC",
    "outputId": "b99627d2-2fad-4828-a29f-eb8b0720085d"
   },
   "outputs": [],
   "source": [
    "# Quick preview\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "HqEY1fQASdWy",
    "outputId": "98e6dd02-95ac-44bd-c2e9-acd427c79211"
   },
   "outputs": [],
   "source": [
    "scaled_df[['course_name_enc', 'course_name']].drop_duplicates().sort_values('course_name_enc').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "Jlw7XDm-TAsy",
    "outputId": "8c93e82f-0ae8-40a9-cf6f-0966560b49a0"
   },
   "outputs": [],
   "source": [
    "scaled_df[['instructor_enc', 'instructor']].drop_duplicates().sort_values('instructor_enc').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "0kIYa5iBTfkz",
    "outputId": "fabbdb70-abd0-4ed3-96f0-e03c0f2cb4cf"
   },
   "outputs": [],
   "source": [
    "scaled_df[['difficulty_level_enc', 'difficulty_level']].drop_duplicates().sort_values('difficulty_level_enc').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcePjbHlZWqV"
   },
   "source": [
    "### PCA & Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Qe6vymoWYeZx",
    "outputId": "03e88f67-0296-4a60-9104-47ee520d6d85"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- Step 1: Define features for PCA ---\n",
    "# We'll use encoded categorical and scaled numeric columns (no raw strings)\n",
    "feature_cols = [\n",
    "    'course_name_enc',\n",
    "    'instructor_enc',\n",
    "    'difficulty_level_enc',\n",
    "    'enrollment_numbers',\n",
    "    'course_price',\n",
    "    'course_duration_hours',\n",
    "    'feedback_score',\n",
    "    'time_spent_hours',\n",
    "    'rating',\n",
    "    'course_taken'\n",
    "]\n",
    "\n",
    "X = scaled_df[feature_cols]\n",
    "\n",
    "# --- Step 2: Apply PCA ---\n",
    "pca = PCA(n_components=None, random_state=42)  # get all components to analyze variance\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# --- Step 3: Create a DataFrame of principal components ---\n",
    "pca_df = pd.DataFrame(\n",
    "    X_pca,\n",
    "    columns=[f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    ")\n",
    "\n",
    "# Add course name (optional for labeling)\n",
    "if 'course_name_enc' in scaled_df.columns:\n",
    "    pca_df['course_name'] = scaled_df['course_name_enc']\n",
    "\n",
    "# --- Step 4: Explained Variance Analysis ---\n",
    "explained_var_ratio = pca.explained_variance_ratio_\n",
    "cumulative_var = np.cumsum(explained_var_ratio)\n",
    "\n",
    "print(\"Explained variance ratio per component:\")\n",
    "for i, var in enumerate(explained_var_ratio):\n",
    "    print(f\"PC{i+1}: {var:.4f} ({cumulative_var[i]*100:.2f}% cumulative)\")\n",
    "\n",
    "# --- Step 5: Plot variance explained ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(explained_var_ratio) + 1), cumulative_var, marker='o')\n",
    "plt.title('Cumulative Explained Variance by PCA Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Step 6: 2D Visualization using first 2 principal components ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x=pca_df['PC1'],\n",
    "    y=pca_df['PC2'],\n",
    "    s=100,\n",
    "    color='teal',\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Optional: label some points\n",
    "for i, txt in enumerate(pca_df['course_name'][:10]):\n",
    "    plt.annotate(txt, (pca_df['PC1'][i], pca_df['PC2'][i]), fontsize=8)\n",
    "\n",
    "plt.title('PCA Visualization (First Two Components)')\n",
    "plt.xlabel(f\"PC1 ({explained_var_ratio[0]*100:.1f}% var)\")\n",
    "plt.ylabel(f\"PC2 ({explained_var_ratio[1]*100:.1f}% var)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Step 7: Feature importance (PCA loadings) ---\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(X.shape[1])],\n",
    "    index=feature_cols\n",
    ")\n",
    "\n",
    "print(\"\\nTop feature contributions to the first few PCs:\")\n",
    "print(loadings.iloc[:, :3])  # top 3 components\n",
    "\n",
    "# Optional: visualize feature importance heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(loadings.iloc[:, :5], annot=True, cmap='viridis')\n",
    "plt.title('Feature Loadings for First 5 Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RIa0qGCFjtZ",
    "outputId": "43529a15-f6c5-42a7-d182-34561289e8ee"
   },
   "outputs": [],
   "source": [
    "!pip install implicit -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCA6RDCoqa6Q"
   },
   "source": [
    "## Model Building\n",
    "### 1. COLLABORATIVE FILTERING (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 761
    },
    "id": "kEQJ9ZW1r8l5",
    "outputId": "7ce8a1c0-cf4e-481d-b45c-9ea3a0320354"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import implicit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_cf = scaled_df.copy()\n",
    "\n",
    "# ---- Step 1: Rebuild interaction (scaled positive values) ----\n",
    "df_cf['interaction'] = (\n",
    "    0.7 * (df_cf['feedback_score'] - df_cf['feedback_score'].min()) /\n",
    "          (df_cf['feedback_score'].max() - df_cf['feedback_score'].min() + 1e-6)\n",
    "    +\n",
    "    0.3 * (df_cf['time_spent_hours'] - df_cf['time_spent_hours'].min()) /\n",
    "          (df_cf['time_spent_hours'].max() - df_cf['time_spent_hours'].min() + 1e-6)\n",
    ")\n",
    "\n",
    "# ---- Step 2: User‚Äìitem mapping ----\n",
    "user_map = {u: i for i, u in enumerate(df_cf['user_id'].unique())}\n",
    "item_map = {c: i for i, c in enumerate(df_cf['course_name_enc'].unique())}\n",
    "user_inv = {i: u for u, i in user_map.items()}\n",
    "item_inv = {i: c for c, i in item_map.items()}\n",
    "\n",
    "rows = df_cf['user_id'].map(user_map)\n",
    "cols = df_cf['course_name_enc'].map(item_map)\n",
    "vals = df_cf['interaction']\n",
    "interaction_matrix = csr_matrix((vals, (rows, cols)),\n",
    "                                shape=(len(user_map), len(item_map)))\n",
    "\n",
    "# ---- Step 3: Train/test split ----\n",
    "train_df, test_df = train_test_split(df_cf, test_size=0.2, random_state=42)\n",
    "train_rows = train_df['user_id'].map(user_map)\n",
    "train_cols = train_df['course_name_enc'].map(item_map)\n",
    "train_vals = train_df['interaction']\n",
    "test_rows = test_df['user_id'].map(user_map)\n",
    "test_cols = test_df['course_name_enc'].map(item_map)\n",
    "test_vals = test_df['interaction']\n",
    "\n",
    "train_matrix = csr_matrix((train_vals, (train_rows, train_cols)),\n",
    "                          shape=(len(user_map), len(item_map)))\n",
    "test_matrix = csr_matrix((test_vals, (test_rows, test_cols)),\n",
    "                         shape=(len(user_map), len(item_map)))\n",
    "\n",
    "# ---- Step 4: Helper functions ----\n",
    "def recommend_cf(model, user_id, n=10):\n",
    "    if user_id not in user_map:\n",
    "        return \"User not found.\"\n",
    "    uid = user_map[user_id]\n",
    "    recs = model.recommend(uid, train_matrix[uid], N=n)\n",
    "    if isinstance(recs, tuple):\n",
    "        item_ids, scores = recs\n",
    "    else:\n",
    "        item_ids, scores = zip(*recs)\n",
    "    course_ids = [item_inv[i] for i in item_ids]\n",
    "\n",
    "    # üîç Map encoded course IDs back to actual names\n",
    "    course_lookup = df_cf[['course_name_enc', 'course_name']].drop_duplicates()\n",
    "    recs_df = pd.DataFrame({'course_name_enc': course_ids, 'score': scores})\n",
    "    recs_df = recs_df.merge(course_lookup, on='course_name_enc', how='left')\n",
    "\n",
    "    # Arrange columns nicely\n",
    "    recs_df = recs_df[['course_name_enc', 'course_name', 'score']]\n",
    "    return recs_df\n",
    "\n",
    "def precision_at_k(model, user_id, k=10):\n",
    "    true_items = set(test_df[test_df['user_id'] == user_id]['course_name_enc'])\n",
    "    if not true_items:\n",
    "        return np.nan\n",
    "    recs = recommend_cf(model, user_id, n=k)\n",
    "    recommended_items = set(recs['course_name_enc'])\n",
    "    return len(true_items.intersection(recommended_items)) / k\n",
    "\n",
    "def recall_at_k(model, user_id, k=10):\n",
    "    true_items = set(test_df[test_df['user_id'] == user_id]['course_name_enc'])\n",
    "    if not true_items:\n",
    "        return np.nan\n",
    "    recs = recommend_cf(model, user_id, n=k)\n",
    "    recommended_items = set(recs['course_name_enc'])\n",
    "    return len(true_items.intersection(recommended_items)) / len(true_items)\n",
    "\n",
    "\n",
    "# ---- Step 5: Light hyperparameter tuning ----\n",
    "param_grid = [\n",
    "    {'factors': 64, 'regularization': 0.05, 'iterations': 25},\n",
    "    {'factors': 96, 'regularization': 0.05, 'iterations': 25},\n",
    "    {'factors': 128, 'regularization': 0.08, 'iterations': 30},\n",
    "]\n",
    "\n",
    "results = []\n",
    "sample_users = np.random.choice(df_cf['user_id'].unique(), size=30, replace=False)\n",
    "\n",
    "for params in param_grid:\n",
    "    print(f\"\\nüöÄ Training ALS: factors={params['factors']}, reg={params['regularization']}, iters={params['iterations']}\")\n",
    "    model = implicit.als.AlternatingLeastSquares(\n",
    "        factors=params['factors'],\n",
    "        regularization=params['regularization'],\n",
    "        iterations=params['iterations'],\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(train_matrix)\n",
    "\n",
    "    precisions, recalls = [], []\n",
    "    for uid in tqdm(sample_users, desc=\"Evaluating\"):\n",
    "        p = precision_at_k(model, uid, k=10)\n",
    "        r = recall_at_k(model, uid, k=10)\n",
    "        if not np.isnan(p): precisions.append(p)\n",
    "        if not np.isnan(r): recalls.append(r)\n",
    "\n",
    "    mean_p, mean_r = np.mean(precisions), np.mean(recalls)\n",
    "    f1 = 2 * (mean_p * mean_r) / (mean_p + mean_r + 1e-6)\n",
    "    results.append({**params, 'precision': mean_p, 'recall': mean_r, 'f1': f1})\n",
    "\n",
    "# ---- Step 6: Show results ----\n",
    "results_df = pd.DataFrame(results).sort_values('f1', ascending=False)\n",
    "print(\"\\nüèÜ Best parameters found:\")\n",
    "print(results_df.head(3))\n",
    "\n",
    "best_params = results_df.iloc[0]\n",
    "\n",
    "# ---- Step 7: Final model ----\n",
    "model_cf_final = implicit.als.AlternatingLeastSquares(\n",
    "    factors=int(best_params['factors']),\n",
    "    regularization=float(best_params['regularization']),\n",
    "    iterations=int(best_params['iterations']),\n",
    "    random_state=42\n",
    ")\n",
    "model_cf_final.fit(train_matrix)\n",
    "\n",
    "# ---- Step 8: Evaluate final model ----\n",
    "sample_users_final = np.random.choice(df_cf['user_id'].unique(), size=40, replace=False)\n",
    "precisions, recalls = [], []\n",
    "for uid in tqdm(sample_users_final, desc=\"Final Evaluation\"):\n",
    "    p = precision_at_k(model_cf_final, uid, k=10)\n",
    "    r = recall_at_k(model_cf_final, uid, k=10)\n",
    "    if not np.isnan(p): precisions.append(p)\n",
    "    if not np.isnan(r): recalls.append(r)\n",
    "\n",
    "print(f\"\\n‚úÖ Final ALS Model:\")\n",
    "print(f\"Precision@10: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall@10:    {np.mean(recalls):.4f}\")\n",
    "\n",
    "# ---- Step 9: Example Recommendations ----\n",
    "example_user = df_cf['user_id'].iloc[0]\n",
    "recs = recommend_cf(model_cf_final, example_user, n=10)\n",
    "print(f\"\\nüéØ Top 10 Recommendations for User {example_user}:\\n\")\n",
    "print(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 840
    },
    "id": "Nb9JKLODd5RH",
    "outputId": "f05abe13-f72f-422b-d6ef-436e5399f1d2"
   },
   "outputs": [],
   "source": [
    "# VISUALIZING ALS EMBEDDINGS (FIXED FOR GPU)\n",
    "\n",
    "# Convert implicit GPU matrices to CPU NumPy arrays if necessary\n",
    "def to_numpy(matrix):\n",
    "    \"\"\"Converts implicit.gpu._cuda.Matrix or numpy array to numpy.ndarray\"\"\"\n",
    "    try:\n",
    "        return np.array(matrix.to_numpy())\n",
    "    except AttributeError:\n",
    "        return np.array(matrix)\n",
    "\n",
    "# Extract latent factors safely\n",
    "user_factors = to_numpy(model_cf.user_factors)\n",
    "item_factors = to_numpy(model_cf.item_factors)\n",
    "\n",
    "print(f\"‚úÖ User factors shape: {user_factors.shape}\")\n",
    "print(f\"‚úÖ Item factors shape: {item_factors.shape}\")\n",
    "\n",
    "# Dimensionality reduction for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "item_pca = pca.fit_transform(item_factors)\n",
    "user_pca = pca.transform(user_factors)\n",
    "\n",
    "# Prepare data for plotting\n",
    "item_df = pd.DataFrame(item_pca, columns=['x', 'y'])\n",
    "item_df['type'] = 'course'\n",
    "item_df['course_name_enc'] = list(item_inv.values())\n",
    "\n",
    "user_df = pd.DataFrame(user_pca, columns=['x', 'y'])\n",
    "user_df['type'] = 'user'\n",
    "user_df['user_id'] = list(user_inv.values())\n",
    "\n",
    "# Plot user‚Äìcourse latent embeddings\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(item_df['x'], item_df['y'], s=60, c='royalblue', alpha=0.6, label='Courses')\n",
    "plt.scatter(user_df['x'], user_df['y'], s=30, c='orangered', alpha=0.4, label='Users')\n",
    "plt.title('User‚ÄìCourse Embedding Space (ALS Latent Factors)')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Helper: find closest courses to a given user\n",
    "def show_user_neighbors(user_id, top_k=5):\n",
    "    \"\"\"Show the nearest courses in latent space to a given user.\"\"\"\n",
    "    if user_id not in user_map:\n",
    "        return \"User not found.\"\n",
    "\n",
    "    uid = user_map[user_id]\n",
    "    user_vec = user_factors[uid].reshape(1, -1)\n",
    "    sims = np.dot(item_factors, user_vec.T).flatten()\n",
    "    top_idx = np.argsort(-sims)[:top_k]\n",
    "    top_courses = [item_inv[i] for i in top_idx]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'course_name_enc': top_courses,\n",
    "        'similarity': sims[top_idx]\n",
    "    })\n",
    "\n",
    "# Example usage:\n",
    "neighbors = show_user_neighbors(15796, top_k=5)\n",
    "print(\"\\nüéØ Courses closest to user 15796 in latent space:\\n\")\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G9O0fKHZsyv"
   },
   "source": [
    "### 2. CONTENT-BASED FILTERING (CBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9h_83RBTzzfj",
    "outputId": "8bbcde21-f310-4784-8610-9f8ebe8db352"
   },
   "outputs": [],
   "source": [
    "# CONTENT-BASED FILTERING (CBF)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import ndcg_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Step 1Ô∏è‚É£: Prepare feature matrix ---\n",
    "course_features = scaled_df[[\n",
    "    'course_name_enc', 'instructor_enc', 'difficulty_level_enc',\n",
    "    'enrollment_numbers', 'course_price', 'course_duration_hours',\n",
    "    'feedback_score', 'time_spent_hours'\n",
    "]].drop_duplicates(subset=['course_name_enc']).reset_index(drop=True)\n",
    "\n",
    "# Extract feature matrix (everything except course_name_enc)\n",
    "feature_matrix = course_features.drop(columns=['course_name_enc']).values\n",
    "\n",
    "# 2Ô∏è‚É£ Function: Generate recommendations\n",
    "def recommend_cbf(input_features, n=10, feature_weights=None):\n",
    "    \"\"\"\n",
    "    Recommends top-N courses similar to user‚Äôs input profile.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_features : dict\n",
    "        Feature values of the user's interests.\n",
    "    n : int\n",
    "        Number of recommendations.\n",
    "    feature_weights : list or np.ndarray\n",
    "        Optional weights to emphasize certain features.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with top-N recommended courses and similarity scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert input features to array\n",
    "    user_vector = np.array([[\n",
    "        input_features['instructor_enc'],\n",
    "        input_features['difficulty_level_enc'],\n",
    "        input_features['enrollment_numbers'],\n",
    "        input_features['course_price'],\n",
    "        input_features['course_duration_hours'],\n",
    "        input_features['feedback_score'],\n",
    "        input_features['time_spent_hours']\n",
    "    ]])\n",
    "\n",
    "    # Apply feature weights (if any)\n",
    "    if feature_weights is not None:\n",
    "        user_vector = user_vector * feature_weights\n",
    "        weighted_features = feature_matrix * feature_weights\n",
    "    else:\n",
    "        weighted_features = feature_matrix\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sims = cosine_similarity(user_vector, weighted_features)[0]\n",
    "\n",
    "    # Get top-N similar courses\n",
    "    top_indices = sims.argsort()[-n:][::-1]\n",
    "    results = course_features.iloc[top_indices][['course_name_enc']].copy()\n",
    "    results['similarity_score'] = sims[top_indices]\n",
    "\n",
    "    # üß† Add original course names\n",
    "    course_lookup = scaled_df[['course_name_enc', 'course_name']].drop_duplicates()\n",
    "    results = results.merge(course_lookup, on='course_name_enc', how='left')\n",
    "\n",
    "    # Arrange columns nicely\n",
    "    results = results[['course_name_enc', 'course_name', 'similarity_score']]\n",
    "\n",
    "    return results.reset_index(drop=True)\n",
    "\n",
    "# 3Ô∏è‚É£ Evaluation Metrics for CBF\n",
    "def evaluate_cbf(user_id, k=10, feature_weights=None):\n",
    "    \"\"\"\n",
    "    Evaluates CBF recommendations for a user by comparing recommended\n",
    "    courses with courses actually taken or rated highly by the user.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get user‚Äôs profile (average of features of courses they liked/took)\n",
    "    user_courses = scaled_df[scaled_df['user_id'] == user_id]\n",
    "    if user_courses.empty:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    # Construct average profile\n",
    "    input_features = {\n",
    "        'instructor_enc': user_courses['instructor_enc'].mean(),\n",
    "        'difficulty_level_enc': user_courses['difficulty_level_enc'].mean(),\n",
    "        'enrollment_numbers': user_courses['enrollment_numbers'].mean(),\n",
    "        'course_price': user_courses['course_price'].mean(),\n",
    "        'course_duration_hours': user_courses['course_duration_hours'].mean(),\n",
    "        'feedback_score': user_courses['feedback_score'].mean(),\n",
    "        'time_spent_hours': user_courses['time_spent_hours'].mean(),\n",
    "    }\n",
    "\n",
    "    # Get top-N recommendations\n",
    "    recs = recommend_cbf(input_features, n=k, feature_weights=feature_weights)\n",
    "    recommended_courses = set(recs['course_name_enc'])\n",
    "\n",
    "    # True relevant courses (based on what user actually took)\n",
    "    true_courses = set(user_courses['course_name_enc'])\n",
    "\n",
    "    if not true_courses:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    # Compute metrics\n",
    "    hits = len(true_courses.intersection(recommended_courses))\n",
    "    precision = hits / k\n",
    "    recall = hits / len(true_courses)\n",
    "\n",
    "    # For NDCG@K (ranking quality)\n",
    "    y_true = np.isin(course_features['course_name_enc'], list(true_courses)).astype(int)\n",
    "    y_score = np.zeros_like(y_true, dtype=float)\n",
    "    for idx, cname in enumerate(course_features['course_name_enc']):\n",
    "        if cname in recs['course_name_enc'].values:\n",
    "            y_score[idx] = recs.loc[recs['course_name_enc'] == cname, 'similarity_score'].values[0]\n",
    "    ndcg = ndcg_score([y_true], [y_score])\n",
    "\n",
    "    return precision, recall, ndcg\n",
    "\n",
    "# 4Ô∏è‚É£ Lightweight Hyperparameter Tuning (Feature Weighting)\n",
    "param_grid = [\n",
    "    [1, 1, 1, 1, 1, 1, 1],          # baseline (equal weights)\n",
    "    [1, 1, 1, 0.8, 1.2, 1.2, 1.5],  # emphasize time & feedback\n",
    "    [1, 1, 1, 1.2, 0.8, 1.5, 1.5],  # emphasize feedback & time_spent\n",
    "    [1, 1, 1, 1.5, 1, 1.5, 1.5],    # more on engagement features\n",
    "]\n",
    "\n",
    "sample_users = np.random.choice(scaled_df['user_id'].unique(), size=30, replace=False)\n",
    "results = []\n",
    "\n",
    "for weights in param_grid:\n",
    "    precisions, recalls, ndcgs = [], [], []\n",
    "    for uid in tqdm(sample_users, desc=f\"Testing weights={weights}\"):\n",
    "        p, r, n = evaluate_cbf(uid, k=10, feature_weights=weights)\n",
    "        if not np.isnan(p): precisions.append(p)\n",
    "        if not np.isnan(r): recalls.append(r)\n",
    "        if not np.isnan(n): ndcgs.append(n)\n",
    "\n",
    "    results.append({\n",
    "        'weights': weights,\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'ndcg': np.mean(ndcgs)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('precision', ascending=False)\n",
    "print(\"\\nüèÜ Best feature weight combination found:\")\n",
    "print(results_df.head(3))\n",
    "\n",
    "best_weights = results_df.iloc[0]['weights']\n",
    "\n",
    "# 5Ô∏è‚É£ Final Evaluation with Optimal Weights\n",
    "sample_users_final = np.random.choice(scaled_df['user_id'].unique(), size=50, replace=False)\n",
    "precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "for uid in tqdm(sample_users_final, desc=\"Final CBF Evaluation\"):\n",
    "    p, r, n = evaluate_cbf(uid, k=10, feature_weights=best_weights)\n",
    "    if not np.isnan(p): precisions.append(p)\n",
    "    if not np.isnan(r): recalls.append(r)\n",
    "    if not np.isnan(n): ndcgs.append(n)\n",
    "\n",
    "print(\"\\n‚úÖ Final CBF Model Evaluation Results:\")\n",
    "print(f\"Precision@10: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall@10:    {np.mean(recalls):.4f}\")\n",
    "print(f\"NDCG@10:      {np.mean(ndcgs):.4f}\")\n",
    "\n",
    "# 6Ô∏è‚É£ Example: Personalized Recommendation\n",
    "example_user = scaled_df['user_id'].iloc[0]\n",
    "user_profile = scaled_df[scaled_df['user_id'] == example_user].iloc[0]\n",
    "\n",
    "input_features = {\n",
    "    'instructor_enc': user_profile['instructor_enc'],\n",
    "    'difficulty_level_enc': user_profile['difficulty_level_enc'],\n",
    "    'enrollment_numbers': user_profile['enrollment_numbers'],\n",
    "    'course_price': user_profile['course_price'],\n",
    "    'course_duration_hours': user_profile['course_duration_hours'],\n",
    "    'feedback_score': user_profile['feedback_score'],\n",
    "    'time_spent_hours': user_profile['time_spent_hours']\n",
    "}\n",
    "\n",
    "recommendations = recommend_cbf(input_features, n=10, feature_weights=best_weights)\n",
    "print(f\"\\nüéØ Top 10 CBF Recommendations for User {example_user}:\\n\")\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PE7zeyn_cwry"
   },
   "source": [
    "### 3. HYBRID RECOMMENDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQbbocsoWP_b",
    "outputId": "cf09a34e-963d-4174-dfc7-439e8d2d0352"
   },
   "outputs": [],
   "source": [
    "# HYBRID RECOMMENDER WITH AUTOMATIC WEIGHT TUNING + EVALUATION\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import ndcg_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1Ô∏è‚É£ Hybrid Recommendation Function\n",
    "def recommend_hybrid(user_id, input_features, n=10, w_cf=0.6, w_cbf=0.4):\n",
    "    \"\"\"\n",
    "    Hybrid recommender that fuses:\n",
    "      - Collaborative Filtering (ALS model)\n",
    "      - Content-Based Filtering (feature similarity)\n",
    "    Weighted fusion with normalized ranking for stability.\n",
    "\n",
    "    Args:\n",
    "        user_id: target user for recommendations\n",
    "        input_features: user‚Äôs course interest profile\n",
    "        n: number of recommendations to return\n",
    "        w_cf, w_cbf: weights for CF and CBF components\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Validate user ---\n",
    "    if user_id not in user_map:\n",
    "        return \"User not found.\"\n",
    "\n",
    "    uid = user_map[user_id]\n",
    "\n",
    "    # --- CF Recommendations ---\n",
    "    cf_recs = model_cf_final.recommend(uid, interaction_matrix[uid], N=50)\n",
    "\n",
    "    # Handle implicit version differences\n",
    "    if isinstance(cf_recs, tuple):  # newer implicit\n",
    "        item_ids, scores = cf_recs\n",
    "        cf_courses = [item_inv[i] for i in item_ids]\n",
    "        cf_scores = np.array(scores)\n",
    "    else:  # older implicit\n",
    "        cf_courses = [item_inv[i] for i, _ in cf_recs]\n",
    "        cf_scores = np.array([s for _, s in cf_recs])\n",
    "\n",
    "    df_cf_scores = pd.DataFrame({'course_name_enc': cf_courses, 'cf_score': cf_scores})\n",
    "    df_cf_scores['cf_score_norm'] = df_cf_scores['cf_score'].rank(ascending=False, pct=True)\n",
    "\n",
    "    # --- CBF Recommendations ---\n",
    "    user_vector = np.array([[\n",
    "        input_features['instructor_enc'],\n",
    "        input_features['difficulty_level_enc'],\n",
    "        input_features['enrollment_numbers'],\n",
    "        input_features['course_price'],\n",
    "        input_features['course_duration_hours'],\n",
    "        input_features['feedback_score'],\n",
    "        input_features['time_spent_hours']\n",
    "    ]])\n",
    "\n",
    "    sims = cosine_similarity(user_vector, feature_matrix)[0]\n",
    "    df_cbf_scores = pd.DataFrame({\n",
    "        'course_name_enc': course_features['course_name_enc'].values,\n",
    "        'cbf_score': sims\n",
    "    })\n",
    "    df_cbf_scores['cbf_score_norm'] = df_cbf_scores['cbf_score'].rank(ascending=False, pct=True)\n",
    "\n",
    "    # --- Combine CF + CBF ---\n",
    "    df_hybrid = pd.merge(df_cf_scores, df_cbf_scores, on='course_name_enc', how='outer').fillna(0)\n",
    "    df_hybrid['hybrid_score'] = (\n",
    "        w_cf * df_hybrid['cf_score_norm'] + w_cbf * df_hybrid['cbf_score_norm']\n",
    "    )\n",
    "\n",
    "    df_hybrid = df_hybrid.drop_duplicates(subset=['course_name_enc'])\n",
    "    df_hybrid = df_hybrid.sort_values('hybrid_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # üß† Add actual course names\n",
    "    course_lookup = scaled_df[['course_name_enc', 'course_name']].drop_duplicates()\n",
    "    df_hybrid = df_hybrid.merge(course_lookup, on='course_name_enc', how='left')\n",
    "\n",
    "    # ‚úÖ Keep a clean display\n",
    "    df_hybrid = df_hybrid[['course_name_enc', 'course_name', 'cf_score', 'cbf_score', 'hybrid_score']]\n",
    "\n",
    "    return df_hybrid.head(n)\n",
    "\n",
    "\n",
    "# 2Ô∏è‚É£ Evaluation Function for One User\n",
    "def evaluate_user(user_id, input_features, k=10, w_cf=0.6, w_cbf=0.4):\n",
    "    \"\"\"\n",
    "    Evaluate hybrid recommendations using:\n",
    "      - Precision@K\n",
    "      - Recall@K\n",
    "      - NDCG@K\n",
    "    \"\"\"\n",
    "\n",
    "    recs = recommend_hybrid(user_id, input_features, n=k, w_cf=w_cf, w_cbf=w_cbf)\n",
    "    if isinstance(recs, str):  # if user not found\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    recommended = set(recs['course_name_enc'])\n",
    "    user_data = scaled_df[scaled_df['user_id'] == user_id]\n",
    "    true_items = set(user_data['course_name_enc'])\n",
    "\n",
    "    if not true_items:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    # --- Compute metrics ---\n",
    "    hits = len(true_items.intersection(recommended))\n",
    "    precision = hits / k\n",
    "    recall = hits / len(true_items)\n",
    "\n",
    "    # --- NDCG ---\n",
    "    y_true = np.isin(course_features['course_name_enc'], list(true_items)).astype(int)\n",
    "    y_score = np.zeros_like(y_true, dtype=float)\n",
    "    for idx, cname in enumerate(course_features['course_name_enc']):\n",
    "        if cname in recs['course_name_enc'].values:\n",
    "            y_score[idx] = recs.loc[recs['course_name_enc'] == cname, 'hybrid_score'].values[0]\n",
    "    ndcg = ndcg_score([y_true], [y_score]) if np.sum(y_true) > 0 else np.nan\n",
    "\n",
    "    return precision, recall, ndcg\n",
    "\n",
    "\n",
    "# 3Ô∏è‚É£ Hyperparameter Tuning (Find Optimal CF‚ÄìCBF Weights)\n",
    "def tune_hybrid_weights(user_sample, input_features, w_values=np.linspace(0.1, 0.9, 9)):\n",
    "    \"\"\"\n",
    "    Small grid search to optimize hybrid weight (CF vs CBF) for best Precision@K\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    for w_cf in tqdm(w_values, desc=\"Tuning Hybrid Weights\"):\n",
    "        precisions, recalls, ndcgs = [], [], []\n",
    "        for uid in user_sample:\n",
    "            p, r, n = evaluate_user(uid, input_features, k=10, w_cf=w_cf, w_cbf=1 - w_cf)\n",
    "            if not np.isnan(p): precisions.append(p)\n",
    "            if not np.isnan(r): recalls.append(r)\n",
    "            if not np.isnan(n): ndcgs.append(n)\n",
    "\n",
    "        results.append({\n",
    "            'w_cf': w_cf,\n",
    "            'w_cbf': 1 - w_cf,\n",
    "            'precision': np.mean(precisions),\n",
    "            'recall': np.mean(recalls),\n",
    "            'ndcg': np.mean(ndcgs)\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values('precision', ascending=False)\n",
    "    best_row = results_df.iloc[0]\n",
    "    print(\"\\nüèÜ Optimal Hybrid Weights Found:\")\n",
    "    print(best_row)\n",
    "\n",
    "    return best_row['w_cf'], best_row['w_cbf'], results_df\n",
    "\n",
    "\n",
    "# 4Ô∏è‚É£ Evaluate & Tune Model\n",
    "# Example user input (profile or interest vector)\n",
    "user_input = {\n",
    "    'instructor_enc': 0,\n",
    "    'difficulty_level_enc': 1,\n",
    "    'enrollment_numbers': 0.306893,\n",
    "    'course_price': 0.043729,\n",
    "    'course_duration_hours': 0.329474,\n",
    "    'feedback_score': 0.348930,\n",
    "    'time_spent_hours': 0.336380\n",
    "}\n",
    "\n",
    "# Pick sample users for tuning\n",
    "sample_users = np.random.choice(scaled_df['user_id'].unique(), size=30, replace=False)\n",
    "\n",
    "# Tune optimal CF‚ÄìCBF weights\n",
    "best_w_cf, best_w_cbf, tuning_results = tune_hybrid_weights(sample_users, user_input)\n",
    "\n",
    "\n",
    "# 5Ô∏è‚É£ Final Model Evaluation with Optimal Weights\n",
    "sample_users_final = np.random.choice(scaled_df['user_id'].unique(), size=40, replace=False)\n",
    "precisions, recalls, ndcgs = [], [], []\n",
    "\n",
    "for uid in tqdm(sample_users_final, desc=\"Final Hybrid Evaluation\"):\n",
    "    p, r, n = evaluate_user(uid, user_input, k=10, w_cf=best_w_cf, w_cbf=best_w_cbf)\n",
    "    if not np.isnan(p): precisions.append(p)\n",
    "    if not np.isnan(r): recalls.append(r)\n",
    "    if not np.isnan(n): ndcgs.append(n)\n",
    "\n",
    "print(\"\\n‚úÖ Final Hybrid Model Performance:\")\n",
    "print(f\"Precision@10: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall@10:    {np.mean(recalls):.4f}\")\n",
    "print(f\"NDCG@10:      {np.mean(ndcgs):.4f}\")\n",
    "\n",
    "# 6Ô∏è‚É£ Example Recommendation for One User\n",
    "# Use one of the users from final evaluation\n",
    "example_user = sample_users_final[0]\n",
    "recommendations = recommend_hybrid(example_user, user_input, n=10, w_cf=best_w_cf, w_cbf=best_w_cbf)\n",
    "\n",
    "print(f\"\\nüéØ Top 10 Optimized Hybrid Recommendations for User {example_user}:\\n\")\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqKvsEBcX3ur"
   },
   "source": [
    "Your hybrid model retrieves all relevant courses (Recall = 1.0), ranks them very well (NDCG = 0.82), but only about 2‚Äì3 of the top 10 are exact matches (Precision = 0.23)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O11oZbVQWP4c",
    "outputId": "87aed68e-a1d9-4823-8887-42851cac534c"
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Save all necessary artifacts for deployment (v2)\n",
    "\n",
    "import pickle, os, shutil\n",
    "\n",
    "# 1Ô∏è‚É£ Create deployment directory (new version)\n",
    "os.makedirs(\"deploy_bundle_1\", exist_ok=True)\n",
    "\n",
    "# 2Ô∏è‚É£ Save the trained CF model and content-based assets\n",
    "pickle.dump(model_cf_final, open(\"deploy_bundle_1/model_cf.pkl\", \"wb\"))\n",
    "pickle.dump(feature_matrix, open(\"deploy_bundle_1/feature_matrix.pkl\", \"wb\"))\n",
    "pickle.dump(course_features, open(\"deploy_bundle_1/course_features.pkl\", \"wb\"))\n",
    "\n",
    "# 3Ô∏è‚É£ Save collaborative filtering mappings\n",
    "pickle.dump(user_map, open(\"deploy_bundle_1/user_map.pkl\", \"wb\"))\n",
    "pickle.dump(item_map, open(\"deploy_bundle_1/item_map.pkl\", \"wb\"))\n",
    "pickle.dump(user_inv, open(\"deploy_bundle_1/user_inv.pkl\", \"wb\"))\n",
    "pickle.dump(item_inv, open(\"deploy_bundle_1/item_inv.pkl\", \"wb\"))\n",
    "pickle.dump(interaction_matrix, open(\"deploy_bundle_1/interaction_matrix.pkl\", \"wb\"))\n",
    "\n",
    "# 4Ô∏è‚É£ Save hybrid tuning weights\n",
    "pickle.dump({\"w_cf\": best_w_cf, \"w_cbf\": best_w_cbf}, open(\"deploy_bundle_1/best_weights.pkl\", \"wb\"))\n",
    "\n",
    "# 5Ô∏è‚É£ Include label encoders and scaler if they exist\n",
    "try:\n",
    "    pickle.dump(le_course, open(\"deploy_bundle_1/le_course.pkl\", \"wb\"))\n",
    "    pickle.dump(le_instructor, open(\"deploy_bundle_1/le_instructor.pkl\", \"wb\"))\n",
    "    pickle.dump(le_difficulty, open(\"deploy_bundle_1/le_difficulty.pkl\", \"wb\"))\n",
    "    pickle.dump(scaler, open(\"deploy_bundle_1/minmax_scaler.pkl\", \"wb\"))\n",
    "    print(\"‚úÖ Encoders & scaler saved.\")\n",
    "except NameError:\n",
    "    print(\"‚ÑπÔ∏è Encoders/scaler not found ‚Äî skipping (safe to ignore if not used).\")\n",
    "\n",
    "# 6Ô∏è‚É£ Confirm completion\n",
    "print(\"‚úÖ All core artifacts saved in 'deploy_bundle_1'.\")\n",
    "\n",
    "# 7Ô∏è‚É£ Create a zip archive for deployment (new version)\n",
    "shutil.make_archive(\"hybrid_recommender_bundle_1\", \"zip\", \"deploy_bundle_1\")\n",
    "print(\"üì¶ Created 'hybrid_recommender_bundle_1.zip' for deployment.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
